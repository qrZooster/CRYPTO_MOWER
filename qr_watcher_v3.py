# =====================================================================
# qr_watcher_v3.py ‚Äî Smart START Generator
# created: 2025-10-19
# short description: –£–º–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ START —Ñ–∞–π–ª–æ–≤ —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö
# =====================================================================

import os, json, time, hashlib
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime

# --- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ---
PROJECT = "CRYPTO_MOWER"
REPO_URL = "https://raw.githubusercontent.com/qrZooster/CRYPTO_MOWER/main"
START_DIR = Path("start")
MAX_CHUNK_SIZE = 120 * 1024  # 120KB

# --- –£–ø—Ä–∞–≤–ª—è—é—â–∏–µ –º–∞—Å—Å–∏–≤—ã ---
CORE_FILES = [
    "bb_sys.py", "bb_db.py", "bb_controls.py", "bb_events.py",
    "bb_logger.py", "bb_utils.py", "bb_tg.py", "bb_ws.py",
    "_bb_ws.py", "bb_page.py"
]

WORK_FILES = [
    "qr_watcher_v3.py", "bb_scan_9.py", "bb_app_sys_control.py",
    "tst_controls.py"
]


# --- –£—Ç–∏–ª–∏—Ç—ã ---
def get_file_hash(filepath: Path) -> str:
    return hashlib.md5(filepath.read_bytes()).hexdigest()


def get_project_state_hash() -> str:
    """–í—ã—á–∏—Å–ª—è–µ—Ç —Ö—ç—à —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞ –ø–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º—ã–º —Ñ–∞–π–ª–∞–º"""
    hashes = []

    # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
    for md_file in Path("docs").glob("*.md"):
        hashes.append(f"docs:{md_file.name}:{get_file_hash(md_file)}")

    # –Ø–¥—Ä–æ
    for core_file in CORE_FILES:
        path = Path(core_file)
        if path.exists():
            hashes.append(f"core:{core_file}:{get_file_hash(path)}")

    # –†–∞–±–æ—á–∏–µ —Ñ–∞–π–ª—ã
    for work_file in WORK_FILES:
        path = Path(work_file)
        if path.exists():
            hashes.append(f"work:{work_file}:{get_file_hash(path)}")

    return hashlib.md5("|".join(sorted(hashes)).encode()).hexdigest()


def cleanup_start_files():
    """–ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ start"""
    if START_DIR.exists():
        for file in START_DIR.glob("START*.json"):
            file.unlink()
        print(f"üßπ –û—á–∏—â–µ–Ω–æ {len(list(START_DIR.glob('START*.json')))} —Å—Ç–∞—Ä—ã—Ö START —Ñ–∞–π–ª–æ–≤")
    else:
        START_DIR.mkdir()


def get_file_data(filepath: Path) -> Dict[str, Any]:
    """–°–æ–±–∏—Ä–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª–∞ –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –≤ START"""
    content = filepath.read_text(encoding="utf-8")
    return {
        "name": filepath.name,
        "dir": f"/{filepath.parent.name}" if filepath.parent.name else "/",
        "lines": content.count("\n") + 1,
        "bytes": filepath.stat().st_size,
        "updated": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime(filepath.stat().st_mtime)),
        "content": content
    }


def collect_prioritized_files() -> List[Dict[str, Any]]:
    """–°–æ–±–∏—Ä–∞–µ—Ç —Ñ–∞–π–ª—ã –≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞: docs -> core -> work"""
    files = []

    # 1. –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è (–≤—Å–µ .md —Ñ–∞–π–ª—ã)
    for md_file in Path("docs").glob("*.md"):
        if md_file.name not in ["STRUCTURE.md", "CONTEXT.md"]:  # –ò—Å–∫–ª—é—á–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ
            files.append(get_file_data(md_file))

    # 2. –Ø–¥—Ä–æ (—Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ñ–∞–π–ª—ã)
    for core_file in CORE_FILES:
        path = Path(core_file)
        if path.exists():
            files.append(get_file_data(path))

    # 3. –†–∞–±–æ—á–∏–µ —Ñ–∞–π–ª—ã
    for work_file in WORK_FILES:
        path = Path(work_file)
        if path.exists():
            files.append(get_file_data(path))

    return files


def create_chunks(files: List[Dict[str, Any]]) -> List[List[Dict[str, Any]]]:
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ñ–∞–π–ª—ã –Ω–∞ —á–∞–Ω–∫–∏ –Ω–µ –±–æ–ª–µ–µ MAX_CHUNK_SIZE"""
    chunks = []
    current_chunk = []
    current_size = 0

    for file in files:
        file_size = len(json.dumps(file))  # –†–µ–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –≤ JSON

        if current_size + file_size > MAX_CHUNK_SIZE and current_chunk:
            chunks.append(current_chunk)
            current_chunk = []
            current_size = 0

        current_chunk.append(file)
        current_size += file_size

    if current_chunk:
        chunks.append(current_chunk)

    return chunks


def generate_start_files():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ START —Ñ–∞–π–ª–æ–≤"""
    print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é START —Ñ–∞–π–ª–æ–≤...")

    # 1. –û—á–∏—Å—Ç–∫–∞
    cleanup_start_files()

    # 2. –°–±–æ—Ä —Ñ–∞–π–ª–æ–≤
    all_files = collect_prioritized_files()
    print(f"üì¶ –°–æ–±—Ä–∞–Ω–æ {len(all_files)} —Ñ–∞–π–ª–æ–≤")

    # 3. –ß–∞–Ω–∫–æ–≤–∞–Ω–∏–µ
    chunks = create_chunks(all_files)
    print(f"üìö –°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")

    # 4. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤
    total_files = 0
    for i, chunk in enumerate(chunks):
        filename = "START.json" if i == 0 else f"START_{i}.json"

        data = {
            "meta": {
                "project": PROJECT,
                "part": filename.replace(".json", ""),
                "command": "/start chat" if i == 0 else None,
                "status": "listening" if i == 0 else "pending",
                "total_parts": len(chunks),
                "loaded_parts": i + 1,
                "files_summary": {
                    "docs": len([f for f in chunk if f["dir"] == "/docs"]),
                    "core": len([f for f in chunk if f["name"] in CORE_FILES]),
                    "work": len([f for f in chunk if f["name"] in WORK_FILES]),
                    "total": len(chunk)
                },
                "updated": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
            },
            "files": chunk
        }

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
        output_path = START_DIR / filename
        output_path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")

        total_files += len(chunk)
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω {filename} ({len(chunk)} —Ñ–∞–π–ª–æ–≤, {output_path.stat().st_size / 1024:.1f} KB)")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ö—ç—à —Å–æ—Å—Ç–æ—è–Ω–∏—è
    current_hash = get_project_state_hash()
    (START_DIR / "last_state.hash").write_text(current_hash)

    print(f"üéâ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {total_files}")


def should_regenerate() -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–≤–∞—Ç—å START —Ñ–∞–π–ª—ã"""
    state_file = START_DIR / "last_state.hash"

    if not state_file.exists():
        return True

    last_hash = state_file.read_text().strip()
    current_hash = get_project_state_hash()

    return last_hash != current_hash


def main():
    print("üîÑ QR Watcher v3 –∑–∞–ø—É—â–µ–Ω...")

    while True:
        if should_regenerate():
            print("üîÑ –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∏–∑–º–µ–Ω–µ–Ω–∏—è - –ø–µ—Ä–µ—Å–æ–∑–¥–∞—ë–º START —Ñ–∞–π–ª—ã")
            generate_start_files()
        else:
            current_time = datetime.now().strftime("%Y-%m-%d –π%H:%M:%S")
            print(f"[{current_time}][qr_watcher: no change]")

        time.sleep(5)


if __name__ == "__main__":
    main()

# =====================================================================
# qr_watcher_v3.py üúÇ The End ‚Äî Tradition Core 2025 ‚öôÔ∏è
# =====================================================================