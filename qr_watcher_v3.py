# =====================================================================
# qr_watcher_v3.py ‚Äî Smart START Generator
# created: 2025-10-19
# short description: –£–º–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ START —Ñ–∞–π–ª–æ–≤ —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö
# =====================================================================

import os, json, time, hashlib
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Tuple

# --- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ---
PROJECT = "CRYPTO_MOWER"
REPO_URL = "https://raw.githubusercontent.com/qrZooster/CRYPTO_MOWER/main"
START_DIR = Path("start")
MAX_CHUNK_SIZE = 120 * 1024  # 120KB

# --- –£–ø—Ä–∞–≤–ª—è—é—â–∏–µ –º–∞—Å—Å–∏–≤—ã ---
IGNORE_DIRS = {
    'venv', '.venv', 'env', '__pycache__', '.git', '.idea',
    'log', 'start', 'backups', 'docs', 'build', 'dist'
}
IGNORE_FILES = {'qr_watcher_v3.py'}  # –ò—Å–∫–ª—é—á–µ–Ω–∏—è –∏–∑ –∞–≤—Ç–æ-–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è

CORE_FILES = [
    "bb_sys.py", "bb_application.py", "bb_db.py", "bb_controls.py", "bb_events.py",
    "bb_logger.py", "bb_utils.py", "bb_tg.py", "bb_ws.py",
    "_bb_ws.py", "bb_page.py"
]

WORK_FILES = [
    "qr_watcher_v3.py", "bb_scan_9.py", "bb_app_sys_control.py",
    "del_tst_controls.py"
]

# –•—ç—à –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å–ø–∏—Å–∫–æ–≤ —Ñ–∞–π–ª–æ–≤
CONFIG_HASH = hashlib.md5(json.dumps({"core": CORE_FILES, "work": WORK_FILES}).encode()).hexdigest()


def discover_python_files() -> List[Path]:
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥–∏—Ç –≤—Å–µ .py —Ñ–∞–π–ª—ã –≤ –ø—Ä–æ–µ–∫—Ç–µ"""
    python_files = []

    # –ò—â–µ–º –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞ –∏ –≤ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ö
    for py_file in Path('.').rglob('*.py'):
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        if any(ignore_dir in py_file.parts for ignore_dir in IGNORE_DIRS):
            continue
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º—ã–µ —Ñ–∞–π–ª—ã
        if py_file.name in IGNORE_FILES:
            continue
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª—ã —É–∂–µ –≤ CORE_FILES –∏–ª–∏ WORK_FILES
        if py_file.name in CORE_FILES or py_file.name in WORK_FILES:
            continue

        python_files.append(py_file)

    return python_files


last_state_file = START_DIR / "last_state.hash"


# --- –£—Ç–∏–ª–∏—Ç—ã ---
def get_file_hash(filepath: Path) -> str:
    return hashlib.md5(filepath.read_bytes()).hexdigest()


def get_project_state_hash() -> Tuple[str, Dict[str, str]]:
    """–í—ã—á–∏—Å–ª—è–µ—Ç —Ö—ç—à —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞ –ø–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º—ã–º —Ñ–∞–π–ª–∞–º"""
    hashes = []
    file_hashes = {}

    # –î–æ–±–∞–≤–ª—è–µ–º —Ö—ç—à –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å–ø–∏—Å–∫–æ–≤
    current_config_hash = hashlib.md5(json.dumps({"core": CORE_FILES, "work": WORK_FILES}).encode()).hexdigest()
    hashes.append(f"config:{current_config_hash}")

    # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
    for md_file in Path("docs").glob("*.md"):
        file_hash = get_file_hash(md_file)
        hashes.append(f"docs:{md_file.name}:{file_hash}")
        file_hashes[str(md_file)] = file_hash

    # –Ø–¥—Ä–æ
    for core_file in CORE_FILES:
        path = Path(core_file)
        if path.exists():
            file_hash = get_file_hash(path)
            hashes.append(f"core:{core_file}:{file_hash}")
            file_hashes[str(path)] = file_hash

    # –†–∞–±–æ—á–∏–µ —Ñ–∞–π–ª—ã
    for work_file in WORK_FILES:
        path = Path(work_file)
        if path.exists():
            file_hash = get_file_hash(path)
            hashes.append(f"work:{work_file}:{file_hash}")
            file_hashes[str(path)] = file_hash

    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
    for py_file in discover_python_files():
        file_hash = get_file_hash(py_file)
        hashes.append(f"auto:{py_file.name}:{file_hash}")
        file_hashes[str(py_file)] = file_hash

    project_hash = hashlib.md5("|".join(sorted(hashes)).encode()).hexdigest()
    return project_hash, file_hashes


def cleanup_start_files():
    """–ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ start"""
    if START_DIR.exists():
        for file in START_DIR.glob("START*.json"):
            file.unlink()
        print(f"üßπ –û—á–∏—â–µ–Ω–æ {len(list(START_DIR.glob('START*.json')))} —Å—Ç–∞—Ä—ã—Ö START —Ñ–∞–π–ª–æ–≤")
    else:
        START_DIR.mkdir()


def get_file_data(filepath: Path) -> Dict[str, Any]:
    """–°–æ–±–∏—Ä–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª–∞ –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –≤ START"""
    content = filepath.read_text(encoding="utf-8")
    return {
        "name": filepath.name,
        "dir": f"/{filepath.parent.name}" if filepath.parent.name else "/",
        "lines": content.count("\n") + 1,
        "bytes": filepath.stat().st_size,
        "updated": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime(filepath.stat().st_mtime)),
        "content": content
    }


def collect_prioritized_files() -> List[Dict[str, Any]]:
    """–°–æ–±–∏—Ä–∞–µ—Ç —Ñ–∞–π–ª—ã –≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞: docs -> core -> work"""
    files = []

    # 1. –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è (–≤—Å–µ .md —Ñ–∞–π–ª—ã)
    for md_file in Path("docs").glob("*.md"):
        # –í–∫–ª—é—á–∞–µ–º –í–°–ï .md —Ñ–∞–π–ª—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
        files.append(get_file_data(md_file))

    # 2. –Ø–¥—Ä–æ (—Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ñ–∞–π–ª—ã)
    for core_file in CORE_FILES:
        path = Path(core_file)
        if path.exists():
            files.append(get_file_data(path))

    # 3. –†–∞–±–æ—á–∏–µ —Ñ–∞–π–ª—ã
    for work_file in WORK_FILES:
        path = Path(work_file)
        if path.exists():
            files.append(get_file_data(path))

    # 4. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ Python —Ñ–∞–π–ª—ã
    for py_file in discover_python_files():
        files.append(get_file_data(py_file))

    return files


def create_chunks(files: List[Dict[str, Any]]) -> List[List[Dict[str, Any]]]:
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ñ–∞–π–ª—ã –Ω–∞ —á–∞–Ω–∫–∏ –Ω–µ –±–æ–ª–µ–µ MAX_CHUNK_SIZE"""
    chunks = []
    current_chunk = []
    current_size = 0

    for file in files:
        file_size = len(json.dumps(file))  # –†–µ–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –≤ JSON

        if current_size + file_size > MAX_CHUNK_SIZE and current_chunk:
            chunks.append(current_chunk)
            current_chunk = []
            current_size = 0

        current_chunk.append(file)
        current_size += file_size

    if current_chunk:
        chunks.append(current_chunk)

    return chunks


def generate_start_files():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ START —Ñ–∞–π–ª–æ–≤"""
    print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é START —Ñ–∞–π–ª–æ–≤...")

    # 1. –û—á–∏—Å—Ç–∫–∞
    cleanup_start_files()

    # 2. –°–±–æ—Ä —Ñ–∞–π–ª–æ–≤
    all_files = collect_prioritized_files()
    print(f"üì¶ –°–æ–±—Ä–∞–Ω–æ {len(all_files)} —Ñ–∞–π–ª–æ–≤")

    # 3. –ß–∞–Ω–∫–æ–≤–∞–Ω–∏–µ
    chunks = create_chunks(all_files)
    print(f"üìö –°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")

    # 4. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤
    total_files = 0
    for i, chunk in enumerate(chunks):
        filename = "START.json" if i == 0 else f"START_{i}.json"

        data = {
            "meta": {
                "project": PROJECT,
                "part": filename.replace(".json", ""),
                "command": "/start chat" if i == 0 else None,
                "status": "listening" if i == 0 else "pending",
                "total_parts": len(chunks),
                "loaded_parts": i + 1,
                "files_summary": {
                    "docs": len([f for f in chunk if f["dir"] == "/docs"]),
                    "core": len([f for f in chunk if f["name"] in CORE_FILES]),
                    "work": len([f for f in chunk if f["name"] in WORK_FILES]),
                    "auto": len([f for f in chunk if
                                 f["name"] not in CORE_FILES and f["name"] not in WORK_FILES and f["dir"] != "/docs"]),
                    "total": len(chunk)
                },
                "updated": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
            },
            "files": chunk
        }

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
        output_path = START_DIR / filename
        output_path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")

        total_files += len(chunk)
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω {filename} ({len(chunk)} —Ñ–∞–π–ª–æ–≤, {output_path.stat().st_size / 1024:.1f} KB)")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ö—ç—à —Å–æ—Å—Ç–æ—è–Ω–∏—è
    current_hash, current_file_hashes = get_project_state_hash()
    state_data = {
        'project_hash': current_hash,
        'file_hashes': current_file_hashes
    }
    last_state_file.write_text(json.dumps(state_data, ensure_ascii=False, indent=2), encoding="utf-8")

    print(f"üéâ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {total_files}")


def should_regenerate() -> Tuple[bool, List[str]]:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–≤–∞—Ç—å START —Ñ–∞–π–ª—ã. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (need_regenerate, changed_files)"""
    current_hash, current_file_hashes = get_project_state_hash()

    if not last_state_file.exists():
        return True, list(current_file_hashes.keys())

    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        with open(last_state_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            last_hash = data.get('project_hash', '')
            last_file_hashes = data.get('file_hashes', {})
    except:
        return True, list(current_file_hashes.keys())

    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ö—ç—à –ø—Ä–æ–µ–∫—Ç–∞
    if last_hash != current_hash:
        # –ù–∞—Ö–æ–¥–∏–º –∏–∑–º–µ–Ω—ë–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
        changed_files = []
        for file_path, current_file_hash in current_file_hashes.items():
            last_file_hash = last_file_hashes.get(file_path)
            if last_file_hash != current_file_hash:
                changed_files.append(Path(file_path).name)

        # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º —É–¥–∞–ª—ë–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
        for file_path in last_file_hashes:
            if file_path not in current_file_hashes:
                changed_files.append(f"{Path(file_path).name} [–£–î–ê–õ–Å–ù]")

        return True, changed_files

    return False, []


def main():
    print("üîÑ QR Watcher v3 –∑–∞–ø—É—â–µ–Ω...")

    while True:
        need_regenerate, changed_files = should_regenerate()
        if need_regenerate:
            if changed_files:
                print(f"üîÑ –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ —Ñ–∞–π–ª–∞—Ö: {', '.join(changed_files)}")
            else:
                print("üîÑ –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∏–∑–º–µ–Ω–µ–Ω–∏—è - –ø–µ—Ä–µ—Å–æ–∑–¥–∞—ë–º START —Ñ–∞–π–ª—ã")
            generate_start_files()
        else:
            current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            print(f"[{current_time}][qr_watcher: no change]")

        time.sleep(5)


if __name__ == "__main__":
    main()

# =====================================================================
# qr_watcher_v3.py üúÇ The End ‚Äî Tradition Core 2025 ‚öôÔ∏è
# =====================================================================